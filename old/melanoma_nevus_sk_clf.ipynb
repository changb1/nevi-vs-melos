{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Path tp the images\n",
    "base_path = './melanoma_nevus'\n",
    "\n",
    "train_folder = os.path.join(base_path, 'train')\n",
    "validation_folder = os.path.join(base_path, 'validation')\n",
    "test_folder = os.path.join(base_path, 'test')\n",
    "\n",
    "train_melanoma_folder = os.path.join(train_folder, 'melanoma')\n",
    "train_nevus_folder = os.path.join(train_folder, 'nevus')\n",
    "validation_melanoma_folder = os.path.join(validation_folder, 'melanoma')\n",
    "validation_nevus_folder = os.path.join(validation_folder, 'nevus')\n",
    "test_melanoma_folder = os.path.join(test_folder, 'melanoma')\n",
    "test_nevus_folder = os.path.join(test_folder, 'nevus')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1600 images belonging to 2 classes.\n",
      "Found 400 images belonging to 2 classes.\n",
      "Found 400 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "SIZE = (600,450)\n",
    "\n",
    "# Rescale image values from 0..255 to 0..1\n",
    "train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "#                                    rotation_range=180,\n",
    "#                                    width_shift_range=0.1,\n",
    "#                                    height_shift_range=0.1,\n",
    "#                                    shear_range=0.1,\n",
    "#                                    zoom_range=0.1,\n",
    "                                   horizontal_flip=True,\n",
    "                                   vertical_flip=True,\n",
    "                                   fill_mode='nearest')\n",
    "\n",
    "valid_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_folder, \n",
    "    target_size = (SIZE[0],SIZE[1]),\n",
    "    batch_size = 20,\n",
    "    class_mode = 'binary')\n",
    "\n",
    "valid_generator = valid_datagen.flow_from_directory(\n",
    "    validation_folder, \n",
    "    target_size = (SIZE[0],SIZE[1]), \n",
    "    batch_size = 20,\n",
    "    class_mode = 'binary')\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_folder, \n",
    "    target_size = (SIZE[0],SIZE[1]), \n",
    "    batch_size = 20,\n",
    "    class_mode = 'binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_dft2(dft2):\n",
    "    c = 255 / np.log(1 + max(abs(dft2.flatten())))\n",
    "    result = c * np.log(1 + abs(dft2))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fd_hu_moments(image):\n",
    "#     image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    feature = cv2.HuMoments(cv2.moments(image)).flatten()\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fd_haralick(image):    \n",
    "    # convert the image to grayscale\n",
    "#     gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    # compute the haralick texture feature vector\n",
    "    haralick = mahotas.features.haralick(image).mean(axis=0)\n",
    "    return haralick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fd_histogram(image, mask=None):\n",
    "    # convert the image to HSV color-space\n",
    "#     image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    # compute the color histogram\n",
    "    hist  = cv2.calcHist([image], [0], None, [256], [0,256])\n",
    "    # normalize the histogram\n",
    "    cv2.normalize(hist, hist)\n",
    "    hist.flatten()\n",
    "    return hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_vector(image):\n",
    "    # FFT\n",
    "    dft = np.fft.fft2(image)\n",
    "    n_dft = norm_dft2(dft)\n",
    "    # Hu moments\n",
    "#     rhm = fd_hu_moments(image)\n",
    "    # \n",
    "#     hl = fd_haralick(image)\n",
    "#     hl = hl / np.max(hl) * 255\n",
    "    #\n",
    "    hist = fd_histogram(image.flatten())\n",
    "    features = np.hstack([n_dft.flatten(), hist.flatten()])\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading images\n",
    "nevus_filepath = 'data/0/'\n",
    "melanoma_filepath = 'data/1/'\n",
    "\n",
    "nevus_files = [f for f in os.listdir(nevus_filepath) if os.path.isfile(nevus_filepath+f)]\n",
    "melanoma_files = [f for f in os.listdir(melanoma_filepath) if os.path.isfile(melanoma_filepath+f)]\n",
    "# sort filenames\n",
    "nevus_files = sorted(nevus_files)\n",
    "melanoma_files = sorted(melanoma_files)\n",
    "\n",
    "N = 500\n",
    "nevus_imgs = [plt.imread(os.path.join(nevus_filepath, nevus_files[i])) for i in range(N)]\n",
    "melanoma_imgs = [plt.imread(os.path.join(melanoma_filepath, melanoma_files[i])) for i in range(N)]\n",
    "\n",
    "# resize\n",
    "nevus_imgs = [cv2.resize(img,(256,256)) for img in nevus_imgs]\n",
    "melanoma_imgs = [cv2.resize(img,(256,256)) for img in melanoma_imgs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/800 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'get_feature_vector' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-0ed6ab44b8bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_feature_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_feature_vector' is not defined"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import mahotas\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "train_len = 1400\n",
    "\n",
    "# Create array with images features\n",
    "X = pd.DataFrame()\n",
    "y_train = np.zeros(1400) + 1\n",
    "y_train[:700] = 0\n",
    "\n",
    "print(\"Train loading...\")\n",
    "fnames = [os.path.join(train_melanoma_folder, fname) for fname in os.listdir(train_melanoma_folder)]\n",
    "for fname in tqdm(fnames): \n",
    "    img = plt.imread(fname) \n",
    "    features = get_feature_vector(img)\n",
    "    X[fname[-11:-4]] = features\n",
    "\n",
    "fnames = [os.path.join(train_nevus_folder, fname) for fname in os.listdir(train_nevus_folder)]\n",
    "for fname in tqdm(fnames): \n",
    "    img = plt.imread(fname) \n",
    "    features = get_feature_vector(img)\n",
    "    X[fname[-11:-4]] = features\n",
    "\n",
    "\n",
    "# # TEST\n",
    "# print(\"Test loading...\")\n",
    "# X_test = pd.DataFrame()\n",
    "# y_test = np.zeros(200) + 1\n",
    "# y_test[:100] = 0\n",
    "\n",
    "# fnames = [os.path.join(test_melanoma_folder, fname) for fname in os.listdir(test_melanoma_folder)]\n",
    "# for fname in tqdm(fnames): \n",
    "#     img = plt.imread(fname)\n",
    "#     features = get_feature_vector(img)\n",
    "#     X_test[fname[-11:-4]] = features\n",
    "\n",
    "# fnames = [os.path.join(test_nevus_folder, fname) for fname in os.listdir(test_nevus_folder)]\n",
    "# for fname in tqdm(fnames): \n",
    "#     img = plt.imread(fname) \n",
    "#     features = get_feature_vector(img)\n",
    "#     X_test[fname[-11:-4]] = features   \n",
    "    \n",
    "    \n",
    "# X_train = X.to_numpy().T\n",
    "# scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "# X_train = scaler.fit_transform(new_X)\n",
    "\n",
    "# X_test = X_test.to_numpy().T\n",
    "# scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "# X_test = scaler.fit_transform(new_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(min_samples_leaf=20, verbose=0, n_jobs=10)\n",
    "clf.fit(new_X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = 0\n",
    "for i in range(new_X_test.shape[0]):\n",
    "    y_p = clf.predict([new_X_test[i]])\n",
    "    if y_p[0] != y[i]:\n",
    "        error += 1\n",
    "print(error/new_X_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADAboost\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "clf = AdaBoostClassifier(n_estimators=100, random_state=0)\n",
    "clf.fit(new_X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "import xgboost as xgb\n",
    "\n",
    "dtrain = xgb.DMatrix(new_X, label=y)\n",
    "dtest = xgb.DMatrix(new_X_test, label=y_test)\n",
    "\n",
    "param = {'max_depth':2, 'eta':1, 'objective':'binary:logistic'}\n",
    "num_round = 10\n",
    "\n",
    "bst = xgb.train(param, dtrain, num_round)\n",
    "\n",
    "preds = bst.predict(dtest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(np.round(preds) == y_test) / len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n",
    "                                 max_depth=1, random_state=0).fit(X_, y)\n",
    "clf.score(new_X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "clf1 = LogisticRegression(random_state=1, max_iter=300)\n",
    "clf2 = RandomForestClassifier(n_estimators=150, random_state=1)\n",
    "clf3 = GaussianNB()\n",
    "\n",
    "N = 400\n",
    "X = np.append(new_X[:700][:N], new_X[700:][:N], axis=0)\n",
    "Y = np.append(y[:700][:N], y[700:][:N], axis=0)\n",
    "\n",
    "eclf = VotingClassifier(estimators=[('lr', clf1), \n",
    "                                    ('rf', clf2), \n",
    "                                    ('gnb', clf3)],\n",
    "                        voting='hard')\n",
    "\n",
    "for clf, label in zip([clf1, clf2, clf3, eclf], ['Logistic Regression', 'Random Forest', 'naive Bayes', 'Ensemble']):\n",
    "    scores = cross_val_score(clf, X, Y, scoring='accuracy', cv=5)\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = new_X[:700][:100]\n",
    "# b = new_X[700:][:100]\n",
    "\n",
    "# np.append(a,b, axis=0).shape\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "melanoma_embedded = TSNE(n_components=3, n_jobs=-1).fit_transform(X_train[:400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nevus_embedded = TSNE(n_components=4, n_jobs=-1).fit_transform(X_train[700:1200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(X_embedded.T[0],X_embedded.T[1], 'o')\n",
    "plt.plot(nevus_embedded.T[0],nevus_embedded.T[1], 'o')\n",
    "# X_embedded.T[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "x1 = melanoma_embedded.T[0]\n",
    "y1 = melanoma_embedded.T[1]\n",
    "z1 = melanoma_embedded.T[2]\n",
    "\n",
    "x2 = nevus_embedded.T[0]\n",
    "y2 = nevus_embedded.T[1]\n",
    "z2 = nevus_embedded.T[2]\n",
    "\n",
    "# t = np.linspace(0, 10, 50)\n",
    "# x, y, z = np.cos(t), np.sin(t), t\n",
    "\n",
    "fig = go.Figure(data=[go.Scatter3d(x=x1,y=y1,z=z1, mode='markers'),\n",
    "                      go.Scatter3d(x=x2,y=y2,z=z2, mode='markers')])\n",
    "fig.update_layout(width=900, height=1000)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distinguish 2 kides of melonomes\n",
    "imgs = []\n",
    "\n",
    "print(\"Train loading...\")\n",
    "fnames = [os.path.join(train_melanoma_folder, fname) for fname in os.listdir(train_melanoma_folder)]\n",
    "for fname in tqdm(fnames): \n",
    "    img = plt.imread(fname) \n",
    "    imgs += [img]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distinguish 2 kides of nevus\n",
    "n_imgs = []\n",
    "\n",
    "print(\"Train loading...\")\n",
    "fnames = [os.path.join(train_nevus_folder, fname) for fname in os.listdir(train_nevus_folder)]\n",
    "for fname in tqdm(fnames): \n",
    "    img = plt.imread(fname) \n",
    "    n_imgs += [img]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(n_imgs[28], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(1,2, figsize=(8,16))\n",
    "# ax[0].imshow(imgs[1], cmap='gray')\n",
    "# ax[1].imshow(imgs[3], cmap='gray')\n",
    "plt.imshow(imgs[29], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "X = np.array(imgs)\n",
    "\n",
    "# X_train = X.reshape(700, X.shape[1]*X.shape[2])\n",
    "\n",
    "X_train = features_train\n",
    "# FFT\n",
    "# X_train = []\n",
    "# for x in X:\n",
    "#     X_train += [np.fft.fft2(x).real]\n",
    "\n",
    "# X_train = np.array(X_train)\n",
    "# X_train = X_train.reshape(700, X_train.shape[1]*X_train.shape[2])\n",
    "\n",
    "ipca = PCA(n_components=2)\n",
    "pca = ipca.fit(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "pca_data = ipca.transform(X_train)\n",
    "\n",
    "df = pd.DataFrame(pca_data)\n",
    "\n",
    "\n",
    "sns.set(style=\"white\")\n",
    "\n",
    "g = sns.PairGrid(df, diag_sharey=False)\n",
    "g.map_upper(sns.scatterplot)\n",
    "g.map_lower(sns.kdeplot, colors=\"C0\")\n",
    "g.map_diag(sns.kdeplot, lw=2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "km = KMeans(init=pca.components_, n_clusters=2, n_init=1, n_jobs=-1, algorithm='full')\n",
    "\n",
    "km.fit(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = km.predict(X_train)\n",
    "\n",
    "x1 = pca_data.T[0][Z==1]\n",
    "y1 = pca_data.T[1][Z==1]\n",
    "\n",
    "x1 = pca_data.T[0][Z==0]\n",
    "y1 = pca_data.T[1][Z==0]\n",
    "\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.plot(x1,y1, '.')\n",
    "plt.plot(x2,y2, '.')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "one_class = []\n",
    "two_class = []\n",
    "\n",
    "for i in range(len(imgs)):\n",
    "    if Z[i] == 0:\n",
    "        one_class += [imgs[i]]\n",
    "    if Z[i] == 1:\n",
    "        two_class += [imgs[i]]\n",
    "        \n",
    "\n",
    "fig, ax = plt.subplots(10,10, figsize=(16,16))\n",
    "\n",
    "io = 0\n",
    "it = 0\n",
    "for i in range(len(ax)):\n",
    "    for j in range(len(ax[i])):\n",
    "        ax[i,j].axis('off')\n",
    "        if i < 5:\n",
    "            ax[i,j].imshow(one_class[io], cmap='Blues')\n",
    "            io += 1\n",
    "        else:\n",
    "            ax[i,j].imshow(two_class[it], cmap='Reds')\n",
    "            it += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 \n",
    "\n",
    "def haralick_for(img):\n",
    "    return mahotas.features.haralick(img).mean(0)\n",
    "\n",
    "def lbp_for(img):\n",
    "    return mahotas.features.lbp(img, 3, 10)\n",
    "\n",
    "def kaze_for(img):\n",
    "    vector_size = 32\n",
    "\n",
    "    alg = cv2.KAZE_create()\n",
    "    kps = alg.detect(img)\n",
    "\n",
    "    kps = sorted(kps, key=lambda x: -x.response)[:vector_size]\n",
    "    kps, dsc = alg.compute(img, kps)\n",
    "\n",
    "    # Descriptor vector size is 64\n",
    "    needed_size = (vector_size * 64)\n",
    "\n",
    "    if type(dsc) == np.ndarray:\n",
    "        dsc = dsc.flatten()\n",
    "        d = np.concatenate([dsc, np.zeros(needed_size - dsc.size)])\n",
    "    else:\n",
    "        d = np.zeros(needed_size)\n",
    "    return d\n",
    "\n",
    "features_train = []\n",
    "\n",
    "for img in tqdm(imgs):\n",
    "    # Filtering image\n",
    "    blur = cv2.GaussianBlur(img,(5,5),0)\n",
    "    ret,t1 = cv2.threshold(blur,127,255,cv2.THRESH_TOZERO)\n",
    "\n",
    "    blur = cv2.GaussianBlur(t1,(5,5),0)\n",
    "    ret3,t4 = cv2.threshold(blur,0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "    \n",
    "    m_img = cv2.bitwise_and(img, img, mask= 255 - t4)\n",
    "    \n",
    "    # Feature from image\n",
    "    features = np.concatenate([haralick_for(m_img), \n",
    "                               lbp_for(m_img), \n",
    "                               kaze_for(m_img), \n",
    "                               np.fft.fft2(m_img).real.flatten()])\n",
    "    features_train += [features]\n",
    "    \n",
    "features_train = np.array(features_train)\n",
    "features_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fig, ax = plt.subplots(10,4, figsize=(16,8*10))\n",
    "for i in range(10):\n",
    "    img = imgs[160+i]\n",
    "    blur = cv2.GaussianBlur(img,(5,5),0)\n",
    "    ret,t1 = cv2.threshold(blur,127,255,cv2.THRESH_TRUNC)\n",
    "\n",
    "    blur = cv2.GaussianBlur(t1,(5,5),0)\n",
    "    ret3,t4 = cv2.threshold(blur,0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "    \n",
    "    m_img = cv2.bitwise_and(img, img, mask= 255 - t4)\n",
    "    \n",
    "    ax[i,0].imshow(img)\n",
    "    ax[i,1].imshow(t1)\n",
    "    ax[i,2].imshow(t4)\n",
    "    ax[i,3].imshow(m_img)\n",
    "    \n",
    "    \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
